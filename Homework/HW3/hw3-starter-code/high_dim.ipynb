{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sopt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to use the scipy optimizers\n",
    "\n",
    "# use float64 by default\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "# Reshape 1D arrays to 2D arrays\n",
    "def reshape_2d(x):\n",
    "    return tf.reshape(x,(x.shape[0],1))\n",
    "\n",
    "def function_factory(model, x_train, y_train, validation_data=None,\n",
    "                     iprint=-1):\n",
    "    \"\"\"A factory to create a function required by scipy.optimize.\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        loss [in]: a function with signature loss_value = loss(pred_y, true_y).\n",
    "        x_train [in]: input for training data.\n",
    "        y_train [in]: output for training data.\n",
    "        validation_data [in]: tuple (x_val,y_val) with validation data.\n",
    "        iprint [in]: sets the frequency with which the loss info is printed out\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = []  # stitch indices\n",
    "    part = []  # partition indices\n",
    "\n",
    "    for i, tensor in enumerate(model.trainable_variables):\n",
    "        n = np.product(tensor.shape)\n",
    "        idx.append(tf.reshape(\n",
    "            tf.range(count, count+n, dtype=tf.int32), tensor.shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # function to calculate loss value and gradient\n",
    "    @tf.function\n",
    "    def tf_tape_grad(params_1d):\n",
    "\n",
    "        # update the parameters in the model\n",
    "        assign_new_model_parameters(params_1d)       \n",
    "\n",
    "        if not (validation_data is None):\n",
    "          # compute validation loss\n",
    "          loss_value = model.loss(validation_data[1], model(validation_data[0]))\n",
    "          # store validation value so we can retrieve later        \n",
    "          tf.py_function(value_and_grad.hist_loss_val.append,\n",
    "                        inp=[loss_value], Tout=[])         \n",
    "              \n",
    "        # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(model.trainable_variables)\n",
    "            # run the model\n",
    "            y_model = model(x_train, training=True)            \n",
    "            # loss value\n",
    "            pred_loss = model.loss(y_train, y_model)\n",
    "            # regularization loss\n",
    "            regularization_loss = tf.cast( tf.reduce_sum(model.losses), tf.float64)            \n",
    "            # total loss\n",
    "            total_value = pred_loss + regularization_loss\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(total_value, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # increment iteration counter\n",
    "        value_and_grad.iter.assign_add(1)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        if (iprint >= 0 and value_and_grad.iter % iprint == 0):\n",
    "            tf.print(\"Loss function eval:\", value_and_grad.iter,\n",
    "                     \"loss:\", pred_loss, \n",
    "                     \"loss_reg:\", regularization_loss,\n",
    "                     \"total:\", total_value)\n",
    "\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(value_and_grad.hist_loss.append,\n",
    "                       inp=[total_value], Tout=[])       \n",
    "\n",
    "        return total_value, grads\n",
    "\n",
    "    # create function that will be returned by this factory\n",
    "    def value_and_grad(params_1d):\n",
    "        \"\"\"A function that can be used by optimizer.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "        return [vv.numpy().astype(np.float64) for vv in tf_tape_grad(tf.constant(params_1d, dtype=tf.float64))]\n",
    "\n",
    "    # store this information as members so we can use it outside the scope\n",
    "    value_and_grad.iter = tf.Variable(0)\n",
    "    value_and_grad.idx = idx\n",
    "    value_and_grad.part = part\n",
    "    value_and_grad.shapes = shapes\n",
    "    value_and_grad.assign_new_model_parameters = assign_new_model_parameters\n",
    "    value_and_grad.hist_loss = []\n",
    "    value_and_grad.hist_loss_val = []\n",
    "\n",
    "    return value_and_grad\n",
    "\n",
    "\n",
    "def model_fit(model, x_t, y_t, validation_data=None, epochs=1000, iprint=-1,\n",
    "              figname=None):\n",
    "    \"\"\" \n",
    "    Fit a DNN model using scipy optimizers \n",
    "  \n",
    "  \n",
    "    Parameters: \n",
    "    model: tensorflow DNN model\n",
    "    x_t: input training data\n",
    "    y_t: output training data\n",
    "    validation_data: tuple (x_val,y_val) with validation data\n",
    "    epochs: maximum number of iterations in optimizer\n",
    "    iprint: frequency for printing the loss function information. \n",
    "            Do not print anything if negative. Otherwise, print\n",
    "            a line every iprint iteration.\n",
    "    figname [str]: file name to save the figure of the training loss  \n",
    "    \"\"\"\n",
    "\n",
    "    value_and_grad = function_factory(model, x_t, y_t, validation_data,\n",
    "                                      iprint)\n",
    "\n",
    "    # convert initial model parameters to a 1D tf.Tensor\n",
    "    init_params = tf.dynamic_stitch(\n",
    "        value_and_grad.idx, model.trainable_variables)\n",
    "\n",
    "    if (iprint>=0):\n",
    "        print()\n",
    "\n",
    "    # train the model\n",
    "    method = 'L-BFGS-B'\n",
    "    results = sopt.minimize(fun=value_and_grad, x0=init_params,\n",
    "                            jac=True, method=method,\n",
    "                            options={'maxiter': epochs})\n",
    "\n",
    "    print(\"\\nConvergence information:\")\n",
    "    print('loss:', results.fun)\n",
    "    print('number function evaluations:', results.nfev)\n",
    "    print('number iterations:', results.nit)\n",
    "    print('success flag:', results.success)\n",
    "    print('convergence message:', results.message)\n",
    "\n",
    "    value_and_grad.assign_new_model_parameters(results.x)\n",
    "\n",
    "    # Plot history of loss\n",
    "    plt.figure()   \n",
    "    plt.plot(value_and_grad.hist_loss, label='loss')\n",
    "    if not (validation_data is None): \n",
    "        plt.plot(value_and_grad.hist_loss_val, label='validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.yscale('log')\n",
    "    if x_val is None:    \n",
    "        plt.title('Training loss')\n",
    "    else:\n",
    "        plt.title('Training and validation losses')\n",
    "\n",
    "    if not (figname is None):\n",
    "        plt.savefig(figname,dpi=300)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function we are trying to learn\n",
    "def f(x, u):\n",
    "    return np.exp(-(x.dot(u)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of dimensions\n",
    "d = 300\n",
    "# Load u vector and normalize it\n",
    "u = np.load(\"u.npy\")\n",
    "u /= np.linalg.norm(u, axis=0)\n",
    "\n",
    "# Codes to generate training and regular validation set. DO NOT modify this cell.\n",
    "\n",
    "# Number of training samples\n",
    "N_train = 10000\n",
    "N_val = 1000\n",
    "\n",
    "# Training set\n",
    "x_train = np.asarray([np.random.normal(size=d) for _ in range(N_train)])\n",
    "y_train = f(x_train,u)\n",
    "\n",
    "# Validation set\n",
    "x_val = np.asarray([np.random.normal(size=d) for _ in range(N_val)])\n",
    "y_val = f(x_val,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build you model and train it using the function model_fit() provided above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written answers for Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written answers for Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written answers for Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate data along different lines parallel and orthogonal to u\n",
    "\n",
    "n_lines = 4\n",
    "n_sample_plot = 128\n",
    "\n",
    "# dir_otg are random directions orthogonal to u\n",
    "dir_otg = np.random.rand(d, n_lines)\n",
    "# x = x - (x.u) * u\n",
    "dir_otg -= np.squeeze( np.transpose(u).dot(dir_otg) ) * u\n",
    "dir_otg /= np.linalg.norm(dir_otg, axis=0) # normalize vectors\n",
    "    \n",
    "# Adding a shift or bias    \n",
    "bias = np.random.uniform(size=(n_lines)) * 2 - 1\n",
    "\n",
    "def uniform_distribution(j,n,scale):\n",
    "    return (2*j/n-1)*scale\n",
    "\n",
    "# Generate different lines, where in each line the variance of data is orthogonal to u \n",
    "x_val_otg = np.zeros( (n_lines, n_sample_plot, d) )\n",
    "for i in range(n_lines):\n",
    "    for j in range(n_sample_plot):\n",
    "        x_val_otg[i,j,:] = bias[i] * np.squeeze(u) + uniform_distribution(j,n_sample_plot,1) * dir_otg[:,i]\n",
    "\n",
    "# Generate different lines, where in each line the variance of data is parallel with u \n",
    "x_val_prl = np.zeros( (n_lines, n_sample_plot, d) )\n",
    "for i in range(n_lines):\n",
    "    for j in range(n_sample_plot):\n",
    "        x_val_prl[i,j,:] = bias[i] * dir_otg[:,i] + uniform_distribution(j,n_sample_plot,1) * np.squeeze(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted and ground-truth values for all lines\n",
    "# color = ['r','b','g','k']\n",
    "# x_index = np.linspace(-1,1,n_sample_plot)\n",
    "# y_pred = model.predict(x_val_otg[i,:,:])\n",
    "# plt.plot(x_index, y_pred, color[i])\n",
    "# y_true = f(x_val_otg[i,:,:], u)\n",
    "# plt.plot(x_index, y_true, color[i] + '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted and ground-truth values for all lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written answers for Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the experiment of Q4 with a new choice for x_val_prl[i,j,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written answers for Q5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

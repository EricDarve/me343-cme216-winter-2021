---
layout: page
title: Programming Homework 3
---

**Submission instructions**: Please use your Python notebook for your programming and written answers. You can do that by including “text cells” or “markdown cells” in your Python notebook. You can just type in your answers in these cells as text. At the end, you will have one document (the notebook) with all your answers. To submit, save the notebook as PDF and submit this PDF as your answer. We will also accept other formats for your submission if this does not work for you.

[Starter code and required files](https://github.com/EricDarve/me343-cme216-winter-2021/tree/main/Homework/HW3/hw3-starter-code)

In this homework, you will implement and train a neural networks using TensorFlow and Keras to solve a regression problem in high dimension.

Regression in high dimension is in general a very hard problem. Assume for example that you build a uniform Cartesian grid for the unit cube in dimension $d$. You use 10 samples along each direction. The total number of sample points required is then $10^d$, which is quickly intractable even for moderate values of $d$.

For many functions however, although $d$ may be large, the effective dimensionality of the problem may be much smaller. Consider for example a scalar function $f$ and a unit vector $u \in \mathbb R^d$. You can define:

$$g(x) = f(u^T x) $$

Although $g$ takes as input a high dimensional vector $x$ it can still be "learned" because there is only one direction along which $g$ changes, that is $u$. This can be extended to multivariable functions $f$. For example, $f$ may take as input a vector in $\mathbb R^3$, $f(x_1,x_2,x_3)$. In this case, $u$ is replaced by a matrix of dimension $d \times 3$. The columns of $u$ span a subspace called the active subspace. In this case, $g$ only changes when $x$ moves parallel to the active subspace. If it moves in an orthogonal direction, $g$ is constant.

For many problems, although the function $g$ is not exactly of the form given above, it is often the case that there exists a function $f$ and matrix $u$ such that $g(x) \approx f(u^T x)$ is a good approximation.

One of the powerful properties of deep neural networks is that they are able to automatically detect such active subspaces and can provide good models for this class of functions. We will explore some of these ideas in this homework.

We are interested in approximating the following function

$$y = \exp \Big(-\frac{(u^Tx)^2}{d} \Big),$$

using a deep neural network. The input $x \in R^d$ is a high-dimensional vector where $d = 300$ in our case.

Launch `high_dim.ipynb` to get started. The code for data loading has been provided in the beginning of the notebook.

In this homework, to accelerate the training, we will use the optimizer from [SciPy](https://www.scipy.org/) to update the neural network, instead of using TensorFlow optimizer as we did in previous homework. The code on how to use SciPy optimizer has been provided to you in the beginning of `high_dim.ipynb`. For example, to train a `model` with `x_train` and `y_train`, run `model_fit(model, x_train, y_train)`. Check the function signature of `model_fit` to see how to set other training hyper-parameters, e.g. the number of epochs.

{:start="1"}

1. Build a neural network and train it with `x_train` (input) and `y_train` (output). Use _mean squared error_ as the loss function. Similar to what you have done in Homework 2, explore different neural network configurations (e.g., layer depth and width) and regularization. Make sure the training process converges and the model is not overfitted on the validation set `x_val` and `y_val`.

Submission instructions: turn in the code in your notebook to build, compile and train the model. In addition, answer the following questions:
- Report the number of iterations you used.
- Report the regularization technique you used.
- Report the final training and validation losses (`model.evaluate()`).
- Plot the training and validation losses in the same figure (which will be automatically generated by the function `model_fit`).

{:start="2"}

2. If the input $x$ is orthogonal to $u$, what is value of the ground-truth output $y=\exp \Big(-\frac{(u^Tx)^2}{d} \Big)$? We will inspect how your trained model performs for some special inputs. Consider a hyperplane $H \in R^d$ where a vector in this hyperplane can be decomposed as $x_{i} = u_{p} + \alpha_i u_{o}$. $\alpha_i$ is a scalar varying for different $x_{i}$, $u_{p}$ is a vector parallel with $u$, $u_{o}$ is a vector orthogonal to $u$. Samples from four such hyperplanes are provided in the starter code, where `x_val_otg[i, :, :]` represents 128 samples in the $i$th hyperplane. For each hyperplane, plot both the ground-truth and predicted (by your trained model) output value $y$. In the figure, clearly mark which lines are prediction/ground-truths using matplotlib legend. Include plots of all hyperplanes in one figure. Examples codes for plotting are provided in the starter code.

Submission instructions: turn in the code in your notebook for plotting. In addition, answer the following questions:
- The value of $y$ if the input $x$ is orthogonal to $u$.
- The plot including the ground-truth and predicted output value $y$ of all 4 hyperplanes.

{:start="3"}

3. If the input $x$ is parallel with $u$, write the expression of the output $y$ in terms of the $l_2$ norm of $x$ (assuming $u$ is a unit vector). Consider another special type of hyperplane $M \in R^d$. A vector $x_{i} \in M$ can be decomposed as $x_{i} = \alpha_i u_{p} + u_{o}$, where $\alpha_i$, $u_{p}$ and $u_{o}$ have the same definitions as in the previous question. Samples from four such hyperplanes are provided in the starter code, where `x_val_prl[i, :, :]` contains 128 samples in the $i$th hyperplane. For each hyperplane, plot both the ground-truth and predicted value $y$. Include plots of all hyperplanes in one figure. In the figure, clearly mark which lines are prediction/ground-truths using matplotlib legend. Notice that predictions are more inaccurate for input $x_{i}$ with larger $\alpha_i$ (convince yourself this is the case). In 2 or 3 sentences, provide an explanation for this observation.

Submission instructions: turn in the code in your notebook for plotting. In addition, answer the following questions:
- The expression of the output $y$ in terms of $\lVert x \rVert _2$.
- The plot including the ground-truth and predicted output value $y$ of all 4 hyperplanes.
- An explanation in 2 or 3 sentences for the observation of inaccurate prediction.

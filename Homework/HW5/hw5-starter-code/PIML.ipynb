{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter \n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sopt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to use the scipy optimizers\n",
    "# DO NOT modify this cell\n",
    "\n",
    "# use float64 by default\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "# Reshape 1D arrays to 2D arrays\n",
    "def reshape_2d(x):\n",
    "    return tf.reshape(x,(x.shape[0],1))\n",
    "\n",
    "# Construct a function that can be minimized by scipy optimize\n",
    "# starting from a tensorflow model\n",
    "def function_factory(model, x_train, y_train, validation_data=None,\n",
    "                     iprint=-1):\n",
    "    \"\"\"A factory to create a function required by scipy.optimize.\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        loss [in]: a function with signature loss_value = loss(pred_y, true_y).\n",
    "        x_train [in]: input for training data.\n",
    "        y_train [in]: output for training data.\n",
    "        validation_data [in]: tuple (x_val,y_val) with validation data.\n",
    "        iprint [in]: sets the frequency with which the loss info is printed out\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = []  # stitch indices\n",
    "    part = []  # partition indices\n",
    "\n",
    "    for i, tensor in enumerate(model.trainable_variables):\n",
    "        n = np.product(tensor.shape)\n",
    "        idx.append(tf.reshape(\n",
    "            tf.range(count, count+n, dtype=tf.int32), tensor.shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "    \n",
    "    loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # function to calculate loss value and gradient\n",
    "    @tf.function\n",
    "    def tf_tape_grad(params_1d):\n",
    "\n",
    "        # update the parameters in the model\n",
    "        assign_new_model_parameters(params_1d)       \n",
    "\n",
    "        if not (validation_data is None):\n",
    "          # compute validation loss\n",
    "          loss_val = model.loss(validation_data[0], validation_data[1])\n",
    "          # store validation value so we can retrieve later        \n",
    "          tf.py_function(value_and_grad.hist_loss_val.append,\n",
    "                        inp=[loss_val], Tout=[])         \n",
    "              \n",
    "        # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "        with tf.GradientTape() as g:\n",
    "            loss = model.loss(x_train,y_train)\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = g.gradient(loss, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # increment iteration counter\n",
    "        value_and_grad.iter.assign_add(1)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        if (iprint>=1 and value_and_grad.iter%iprint == 0):\n",
    "            if validation_data is None:\n",
    "                tf.print(\"Loss function eval:\", value_and_grad.iter, \"loss:\", loss)\n",
    "            else:\n",
    "                tf.print(\"Loss function eval:\", value_and_grad.iter, \"loss:\", loss,\\\n",
    "                         \"validation loss:\", loss_val)\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(value_and_grad.hist_loss.append,\n",
    "                       inp=[loss], Tout=[])       \n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    # create function that will be returned by this factory\n",
    "    def value_and_grad(params_1d):\n",
    "        \"\"\"A function that can be used by optimizer.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "        return [vv.numpy().astype(np.float64) for vv in tf_tape_grad(tf.constant(params_1d, dtype=tf.float64))]\n",
    "\n",
    "    # store this information as members so we can use it outside the scope\n",
    "    value_and_grad.iter = tf.Variable(0)\n",
    "    value_and_grad.idx = idx\n",
    "    value_and_grad.part = part\n",
    "    value_and_grad.shapes = shapes\n",
    "    value_and_grad.assign_new_model_parameters = assign_new_model_parameters\n",
    "    value_and_grad.hist_loss = []\n",
    "    value_and_grad.hist_loss_val = []\n",
    "\n",
    "    return value_and_grad\n",
    "\n",
    "\n",
    "# Minimize the loss function using a scipy optimizer\n",
    "def model_fit(model, x_t, y_t, validation_data=None, epochs=1000, iprint=-1,\n",
    "              figname=None):\n",
    "    \"\"\" \n",
    "    Fit a DNN model using scipy optimizers \n",
    "  \n",
    "  \n",
    "    Parameters: \n",
    "    model: tensorflow DNN model\n",
    "    x_t: input training data\n",
    "    y_t: output training data\n",
    "    validation_data: tuple (x_val,y_val) with validation data\n",
    "    epochs: maximum number of iterations in optimizer\n",
    "    iprint: frequency for printing the loss function information. \n",
    "            Do not print anything if negative. Otherwise, print\n",
    "            a line every iprint iteration.\n",
    "    figname [str]: file name to save the figure of the training loss  \n",
    "    \"\"\"\n",
    "\n",
    "    value_and_grad = function_factory(model, x_t, y_t, validation_data, iprint)\n",
    "\n",
    "    # convert initial model parameters to a 1D tf.Tensor\n",
    "    init_params = tf.dynamic_stitch(value_and_grad.idx, model.trainable_variables)\n",
    "\n",
    "    if (iprint>=1):\n",
    "        print()\n",
    "\n",
    "    # train the model\n",
    "    method = 'L-BFGS-B'\n",
    "    results = sopt.minimize(fun=value_and_grad, x0=init_params,\n",
    "                            jac=True, method=method,\n",
    "                            options={'maxiter': epochs})\n",
    "\n",
    "    print(\"\\nConvergence information:\")\n",
    "    print('loss:', results.fun)\n",
    "    # Computing the validation loss  \n",
    "    if not (validation_data is None):\n",
    "        val_loss = model.loss(validation_data[0], validation_data[1])\n",
    "        tf.print('validation loss:', val_loss)\n",
    "        # tf.make_ndarray(tf.make_tensor_proto(val_loss))    \n",
    "    print('number function evaluations:', results.nfev)\n",
    "    print('number iterations:', results.nit)\n",
    "    print('success flag:', results.success)\n",
    "    print('convergence message:', results.message)\n",
    "\n",
    "    value_and_grad.assign_new_model_parameters(results.x)\n",
    "\n",
    "    # Plot history of loss\n",
    "    plt.figure()   \n",
    "    plt.plot(value_and_grad.hist_loss, label='loss')\n",
    "    if not (validation_data is None): \n",
    "        plt.plot(value_and_grad.hist_loss_val, label='validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.yscale('log')\n",
    "    if validation_data is None:    \n",
    "        plt.title('Training loss')\n",
    "    else:\n",
    "        plt.title('Training and validation losses')\n",
    "\n",
    "    if not (figname is None):\n",
    "        plt.savefig(figname,dpi=300)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-niagara",
   "metadata": {},
   "source": [
    "### Code for data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for data generation.\n",
    "# DO NOT modify this cell.\n",
    "\n",
    "# u\n",
    "def u_fun(x):\n",
    "    return np.cos(x**2)\n",
    "\n",
    "# du\n",
    "def du_fun(x):\n",
    "    return -2*x*np.sin(x**2)\n",
    "\n",
    "# k\n",
    "def k_fun(x):\n",
    "    return 1 + x**2\n",
    "\n",
    "def a_fun(x):\n",
    "    return x\n",
    "\n",
    "# d/dx( k du/dx ) + a du/dx + u\n",
    "def f_fun(x):\n",
    "    return (-8*x**2 - 2)*np.sin(x**2) + (-4*x**4 - 4*x**2 + 1)*np.cos(x**2)\n",
    "\n",
    "def generate_data(n_u_obs, n_k_obs, n_a_obs, n_phys):\n",
    "    \"\"\"\n",
    "    Generate n_u_obs data examples for u, n_k_obs examples for k, \n",
    "    n_a_obs examples for a, n_phys examples for ode\n",
    "    \"\"\"\n",
    "    \n",
    "    # u observation data\n",
    "    x_u_obs = reshape_2d(np.linspace(-1,1,n_u_obs))\n",
    "    u_obs = u_fun(x_u_obs)\n",
    "    u_obs = reshape_2d(u_obs)\n",
    "\n",
    "    # k observation data\n",
    "    x_k_obs = reshape_2d(np.linspace(-1,1,n_k_obs))\n",
    "    k_obs = k_fun(x_k_obs)\n",
    "    k_obs = reshape_2d(k_obs)\n",
    "\n",
    "    # a observation data\n",
    "    x_a_obs = reshape_2d(np.linspace(-1,1,n_a_obs))\n",
    "    a_obs = a_fun(x_a_obs)\n",
    "    a_obs = reshape_2d(a_obs)\n",
    "\n",
    "    # Physics data\n",
    "    x_phys = reshape_2d(np.linspace(-1,1,n_phys))\n",
    "    y_phys = f_fun(x_phys)\n",
    "    y_phys = reshape_2d(y_phys)\n",
    "    \n",
    "    X_data = [x_u_obs,x_k_obs,x_a_obs,x_phys]\n",
    "    Y_data = [u_obs,k_obs,a_obs,y_phys]    \n",
    "    \n",
    "    return X_data, Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-payroll",
   "metadata": {},
   "source": [
    "# Questions 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PI_NN(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(PI_NN, self).__init__()\n",
    "        \n",
    "        # 1. Define tf keras layers\n",
    "        \n",
    "        # 2. Define the loss function to use later on\n",
    "        \n",
    "        # delete the line below when your implementation is complete\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Forward pass for model, inputs is of size B x 1, where B is batch size\n",
    "        \n",
    "        # delete the line below when your implementation is complete\n",
    "        pass\n",
    "        \n",
    "        # 1. Use layers defined in __init__ for the forward pass\n",
    "        \n",
    "        # 2. Split the output from the final layer into u, k, a:\n",
    "        # u, k, a = tf.split(final_layer_output, num_or_size_splits=3, axis=1)\n",
    "        \n",
    "        return u, k, a\n",
    "\n",
    "    def get_derivatives(self, x_input):\n",
    "        x_input = tf.convert_to_tensor(x_input)\n",
    "        \n",
    "        # Compute u, k, a, du/dx (denoted as variable u_x) and the LHS of ODE (denoted as variable phys)\n",
    "        \n",
    "        # delete the line below when your implementation is complete\n",
    "        pass\n",
    "    \n",
    "        return u, k, a, u_x, phys\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        # u = self(X[0])[0]\n",
    "        # loss_u = self.loss_fun(Y[0], u)\n",
    "        \n",
    "        # k = self(X[1])[1]\n",
    "        # loss_k = ...\n",
    "        \n",
    "        # a = ...\n",
    "        # loss_a = ...\n",
    "        \n",
    "        # rhs_ode = ... (hint: use self.get_derivatives)\n",
    "        # loss_ode = ...\n",
    "        \n",
    "        # loss = weight_u*loss_u + ...\n",
    "        \n",
    "        # delete the line below when your implementation is complete\n",
    "        pass\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-papua",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 1\n",
    "# a lot of data for $k$, $a$, $f$. Limited observation for $u$. Learn $u$.\n",
    "\n",
    "# Initialize model\n",
    "model = PI_NN()\n",
    "model.build((1,1))\n",
    "\n",
    "# Generate training data\n",
    "X_train, Y_train = generate_data(2, 16, 16, 16)\n",
    "X_val, Y_val     = generate_data(31, 31, 31, 31)\n",
    "\n",
    "# TO DO: use the model_fit function to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the solution\n",
    "\n",
    "x_true = reshape_2d(np.linspace(-1,1,32))\n",
    "x_test = reshape_2d(np.linspace(-1,1,128))\n",
    "\n",
    "u, k, a, u_x, phys = model.get_derivatives(x_test)\n",
    "\n",
    "plt.plot(x_test,u,'r',label='u')\n",
    "plt.plot(x_true,u_fun(x_true),'go',label='true')\n",
    "plt.title('Exact solution and DNN model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,k,'r',label='k')\n",
    "plt.plot(x_true,k_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,a,'r',label='a')\n",
    "plt.plot(x_true,a_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,u_x,'r',label='ux')\n",
    "plt.plot(x_true,du_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,phys,'r',label='ode')\n",
    "plt.plot(x_true,f_fun(x_true),'go',label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error\n",
    "\n",
    "plt.plot(x_test, u - u_fun(x_test),'r',label='u')\n",
    "plt.title('Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, k - k_fun(x_test),'r',label='k')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, a - a_fun(x_test),'r',label='a')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, u_x - du_fun(x_test),'r',label='ux')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, phys - f_fun(x_test),'r',label='ode')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-monaco",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 2\n",
    "# a lot of data for $u$ and $f$. Limited observation for $k$ and $a$. Learn $k$ and $a$\n",
    "model = PI_NN()\n",
    "model.build((1,1))\n",
    "\n",
    "X_train, Y_train = generate_data(16, 4, 4, 16)\n",
    "X_val, Y_val = generate_data(31, 31, 31, 31)\n",
    "\n",
    "# TO DO: use the model_fit function to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the solution\n",
    "u, k, a, u_x, _ = model.get_derivatives(x_test)\n",
    "\n",
    "plt.plot(x_test,k,'r',label='k')\n",
    "plt.plot(x_true,k_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,a,'r',label='a')\n",
    "plt.plot(x_true,a_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, u - u_fun(x_test),'r',label='u')\n",
    "plt.title('Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, k - k_fun(x_test),'r',label='k')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, a - a_fun(x_test),'r',label='a')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, u_x - du_fun(x_test),'r',label='ux')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, phys - f_fun(x_test),'r',label='ode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-feeling",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 3\n",
    "# Hybrid. Combine partial observations for all variables\n",
    "model = PI_NN()\n",
    "model.build((1,1))\n",
    "\n",
    "X_train, Y_train = generate_data(8, 8, 8, 8)\n",
    "X_val, Y_val = generate_data(31, 31, 31, 31)\n",
    "\n",
    "# TO DO: use the model_fit function to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the solution\n",
    "u, k, a, u_x, phys = model.get_derivatives(x_test)\n",
    "\n",
    "plt.plot(x_test,u,'r',label='u')\n",
    "plt.plot(x_true,u_fun(x_true),'go',label='true')\n",
    "plt.title('Exact solution and DNN model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,k,'r',label='k')\n",
    "plt.plot(x_true,k_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,a,'r',label='a')\n",
    "plt.plot(x_true,a_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,u_x,'r',label='ux')\n",
    "plt.plot(x_true,du_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test,phys,'r',label='ode')\n",
    "plt.plot(x_true,f_fun(x_true),'go',label='true')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, u - u_fun(x_test),'r',label='u')\n",
    "plt.title('Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, k - k_fun(x_test),'r',label='k')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, a - a_fun(x_test),'r',label='a')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, u_x - du_fun(x_test),'r',label='ux')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_test, phys - f_fun(x_test),'r',label='ode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%    PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}
\usepackage{animate}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{extarrows}

\newcommand{\ChoL}{\mathsf{L}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bM}{\mathbf{M}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\bt}[0]{\bm{\theta}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bzero}{\mathbf{0}}
\renewcommand{\bf}{\mathbf{f}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}[0]{\mathbf{v}}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}


% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{soul}
\newcommand{\red}[1]{\textcolor{red}{#1}}
%
%\usepackage{graphicx} % Allows including images
%\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%
%
%\usepackage{amsthm}
%
%\usepackage{todonotes}
%\usepackage{floatrow}
%
%\usepackage{pgfplots,algorithmic,algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage[toc,page]{appendix}
%\usepackage{float}
%\usepackage{booktabs}
%\usepackage{bm}
%
%\theoremstyle{definition}
%
\newcommand{\RR}[0]{\mathbb{R}}
%
%\newcommand{\bx}{\mathbf{x}}
%\newcommand{\ii}{\mathrm{i}}
%\newcommand{\bxi}{\bm{\xi}}
%\newcommand{\bmu}{\bm{\mu}}
%\newcommand{\bb}{\mathbf{b}}
%\newcommand{\bA}{\mathbf{A}}
%\newcommand{\bJ}{\mathbf{J}}
%\newcommand{\bB}{\mathbf{B}}
%\newcommand{\bM}{\mathbf{M}}
%\newcommand{\bF}{\mathbf{F}}
%
%\newcommand{\by}{\mathbf{y}}
%\newcommand{\bw}{\mathbf{w}}
%\newcommand{\bn}{\mathbf{n}}
%
%\newcommand{\bX}{\mathbf{X}}
%\newcommand{\bY}{\mathbf{Y}}
%\newcommand{\bs}{\mathbf{s}}
%\newcommand{\sign}{\mathrm{sign}}
%\newcommand{\bt}[0]{\bm{\theta}}
%\newcommand{\bc}{\mathbf{c}}
%\newcommand{\bzero}{\mathbf{0}}
%\renewcommand{\bf}{\mathbf{f}}
%\newcommand{\bu}{\mathbf{u}}
%\newcommand{\bv}[0]{\mathbf{v}}

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------------------------------------------
%    TITLE PAGE
%----------------------------------------------------------------------------------------
\usepackage{bm}
\newcommand*{\TakeFourierOrnament}[1]{{%
\fontencoding{U}\fontfamily{futs}\selectfont\char#1}}
\newcommand*{\danger}{\TakeFourierOrnament{66}}

\title[AD]{Automatic Differentiation for Computational Engineering} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[CME 216]{Kailai Xu and Eric Darve} % Your name
%\institute[] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
%{
%%ICME, Stanford University \\ % Your institution for the title page
%%\medskip
%%\textit{kailaix@stanford.edu}\quad \textit{darve@stanford.edu} % Your email address
%}
\date{}% Date, can be changed to a custom date
% Mathematics of PDEs


\begin{document}

%\usebackgroundtemplate{%
%\begin{picture}(0,250)
%\centering
%	{{\includegraphics[width=1.0\paperwidth]{figures/background}}}
%\end{picture}
%  } 
%\usebackgroundtemplate{%
%  \includegraphics[width=\paperwidth,height=\paperheight]{figures/back}} 
\begin{frame}

	\titlepage % Print the title page as the first slide

	%dfa
\end{frame}
%\usebackgroundtemplate{}

\section{Overview}

\begin{frame}
	\frametitle{Overview}

	\begin{itemize}
		\item Gradients are useful in many applications
		      \begin{itemize}
			      \item \red{Mathematical Optimization}
			            $$\min_{x\in\RR^n} \; f(x)$$
			            Using the gradient descent method:
			            $$x_{n+1} = x_n - \alpha_n \nabla f(x_n)$$
			      \item \red{Sensitivity Analysis}
			            $$f(x+\Delta x) \approx f'(x) \Delta x$$
			      \item  \red{Machine Learning}

			            Training a neural network using automatic differentiation (back-propagation).
			      \item \red{Solving Nonlinear Equations} Solve a nonlinear equation $f(x) = 0$ using Newton's method
			            $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$
		      \end{itemize}

	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Terminology}

	\begin{itemize}

		\item Deriving and implementing gradients are a challenging and all-consuming process.

		\item Automatic differentiation: \red{a set of techniques to numerically evaluate the derivative of a function specified by a computer program} (Wikipedia). It also bears other names such as \red{autodiff}, \red{algorithmic differentiation}, \red{computational differentiation}, and \red{back-propagation}.


		\item There are a lot of AD softwares
		      \begin{enumerate}
			      \item TensorFlow and PyTorch: deep learning frameworks in Python
			      \item Adept-2: combined array and automatic differentiation library in \texttt{C++}
			      \item autograd: efficiently derivatives computation of NumPy code.
			      \item ForwardDiff.jl, Zygote.jl: Julia differentiable programming packages
		      \end{enumerate}


		\item This lecture: how to compute gradients using automatic differentiation (AD)

		      \begin{itemize}
			      \item  Forward mode, reverse mode, and AD for implicit solvers
		      \end{itemize}




	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{AD Software}

	\begin{figure}[hbt]
		\includegraphics[width=1.0\textwidth]{figures/ad_performance}
	\end{figure}

	{\scriptsize \url{https://github.com/microsoft/ADBench}}

\end{frame}

\begin{frame}
	\frametitle{Finite Differences}


	$$f'(x) \approx \frac{f(x+h) - f(x)}{h},\quad f'(x)\approx \frac{f(x+h) - f(x-h)}{2h}$$
	\begin{itemize}
		\item Derived from the definition of derivatives
		      \begin{equation*}
			      f'(x) = \lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h}
		      \end{equation*}
		\item Conceptually simple.
		\item Curse of dimensionalties: to compute the gradients of $f:\RR^m \rightarrow \RR$, you need at least $\mathcal{O}(m)$ function evaluations.
		\item Huge numerical error: roundoff error.
	\end{itemize}

\end{frame}


\begin{frame}
	\frametitle{Finite Difference}
	\begin{equation*}
		f(x) = \sin(x) \quad f'(x) = \cos(x) \quad  x_0 = 0.1
	\end{equation*}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/roundoff}
	\end{figure}

\end{frame}

\begin{frame}
	\frametitle{Finite Difference}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/fd_error}
	\end{figure}
	\vspace{-0.5cm}
	\scriptsize{Baydin, A. G., Pearlmutter, B. A., Radul, A. A., \& Siskind, J. M. (2017). Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1), 5595-5637.}
\end{frame}

\begin{frame}
	\frametitle{Symbolic Differentiation }

	\begin{itemize}
		\item Symbolic differentiation computes exact derivatives (gradients): there is no approximation error.
		\item It works by recursively applies simple rules to \textcolor{red}{symbols}
		      \begin{align*}
			      \frac{d}{dx}(c)   & = 0                                 & \frac{d}{dx}(x)  & = 1                                 \\
			      \frac{d}{dx}(u+v) & = \frac{d}{dx}(u) + \frac{d}{dx}(v) & \frac{d}{dx}(uv) & = v\frac{d}{dx}(u)+u\frac{d}{dx}(v) \\
			      \ldots
		      \end{align*}
		      Here $c$ is a variable independent of $x$, and $u, v$ are variables dependent on $x$.

		\item There may not exist convenient expressions for the analytical gradients of some functions. For example, a blackbox function from a third-party library.

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Symbolic Differentiation}
	\begin{itemize}
		\item Symbolic differentiation can lead to complex and redundant expressions
	\end{itemize}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/symbolic}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Automatic Differentiation}

	\begin{itemize}
		\item AD is neither finite difference nor symbolic differentiation.
		\item It works by recursively applies simple rules to \textcolor{red}{values}
		      \begin{align*}
			      \frac{d}{dx}(c)   & = 0                                 & \frac{d}{dx}(x)  & = 1                                 \\
			      \frac{d}{dx}(u+v) & = \frac{d}{dx}(u) + \frac{d}{dx}(v) & \frac{d}{dx}(uv) & = v\frac{d}{dx}(u)+u\frac{d}{dx}(v) \\
			      \ldots
		      \end{align*}
		      Here $c$ is a variable independent of $x$, and $u, v$ are variables dependent on $x$.

		\item It evaluates numerically gradients of ``function units'' using symbolic differentiation, and chains the computed gradients using the chain rule
		      $$\frac{df(g(x))}{dx} = f'(g(x)) g'(x)$$
		\item It is efficient (linear in the cost of computing the function itself) and numerically stable.
	\end{itemize}

\end{frame}


\section{Computational Graph}

\begin{frame}
	\frametitle{Computational Graph}

	\begin{itemize}
		\item The ``language'' for automatic differentiation is computational graph.
		      \begin{itemize}
			      \item The computational graph is a \red{directed acyclic graph (DAG)}.
			      \item Each \red{edge} represents the data: a scalar, a vector, a matrix, or a high dimensional tensor.
			      \item Each \red{node} is a function that consumes several incoming edges and outputs some values.
		      \end{itemize}
		      \begin{minipage}[c]{0.45\textwidth}
			      \begin{align*}
				      J            & = f_4(\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3, \mathbf{u}_4), \\
				      \mathbf{u}_2 & = f_1(\mathbf{u}_1, \bm {\theta}),                             \\
				      \mathbf{u}_3 & = f_2(\mathbf{u}_2, \bm {\theta}),                             \\
				      \mathbf{u}_4 & = f_3(\mathbf{u}_3, \bm {\theta}).
			      \end{align*}
		      \end{minipage}~
		      \begin{minipage}[c]{0.45\textwidth}
			      \includegraphics[width=1.0\textwidth]{figures/adjoint}
		      \end{minipage}
		\item  Let's build a computational graph for computing
		      $$z=\sin(x_1+x_2) + x_2^2x_3$$

	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Building a Computational Graph}

	$$z=\sin(\red{x_1+x_2}) + \red{x_2^2}x_3$$

	\begin{figure}[hbt]
		\includegraphics[width=0.4\textwidth]{figures/fd1}
	\end{figure}


\end{frame}


\begin{frame}
	\frametitle{Building a Computational Graph}

	$$z=\red{\sin(x_1+x_2)} + \red{x_2^2x_3}$$

	\begin{figure}[hbt]
		\includegraphics[width=0.4\textwidth]{figures/fd2}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Building a Computational Graph}

	$$z=\red{\sin(x_1+x_2) + x_2^2x_3}$$

	\begin{figure}[hbt]
		\includegraphics[width=0.4\textwidth]{figures/fd3}
	\end{figure}
\end{frame}



\begin{frame}
	\frametitle{Computing Gradients from a Computational Graph}
	\begin{itemize}
		\item Automatic differentiation works by \textcolor{red}{propagating} gradients in the computational graph.
		\item Two basic modes: forward-mode and backward-mode. Forward-mode propagates gradients in the same direction as forward computation. Backward-mode propagates gradients in the reverse direction of forward computation.
	\end{itemize}
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=1.0\textwidth]{figures/fb}
	\end{figure}

\end{frame}


\begin{frame}
	\frametitle{Computing Gradients from a Computational Graph}
	\begin{itemize}
		\item Different computational graph topologies call for different modes of automatic differentiation.
		      \begin{itemize}
			      \item One-to-many: forward-propagation$\Rightarrow$forward-mode AD.
			            \begin{figure}[hbt]
				            \includegraphics[width=0.6\textwidth]{figures/onetomany}
			            \end{figure}
			      \item Many-to-one: back-propagation$\Rightarrow$reverse-mode AD.
		      \end{itemize}
		      \begin{figure}[hbt]
			      \includegraphics[width=0.6\textwidth]{figures/manytoone}
		      \end{figure}
	\end{itemize}

\end{frame}



\section{Forward Mode}



\begin{frame}
	\frametitle{Automatic Differentiation: Forward Mode AD}

	\begin{itemize}
		\item The forward-mode automatic differentiation uses the chain rule to propagate the gradients.
		      $$\frac{\partial f\circ g (x)}{\partial x} =  f'(g(x)) \red{ g'(x)}$$
		\item Compute in the same order as function evaluation.
		\item Each node in the computational graph
		      \begin{itemize}
			      \item \red{Aggregate} all the gradients from up-streams.
			      \item \red{Forward} the gradient to down-stream nodes.
		      \end{itemize}
	\end{itemize}

	\begin{figure}[hbt]
		\includegraphics[width=0.4\textwidth]{figures/fad}
	\end{figure}


\end{frame}

\begin{frame}
	\frametitle{Example: Forward Mode AD}
	\begin{itemize}
		\item 	Let's consider a specific way for computing
		      \begin{equation*}
			      f(x) = \begin{bmatrix}
				      x^4           \\
				      x^2 + \sin(x) \\
				      -\sin(x)
			      \end{bmatrix}
		      \end{equation*}
	\end{itemize}


	\pause

	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/forwardmode}
	\end{minipage}~
	\begin{minipage}[b]{0.45\textwidth}
		\begin{align*}
			\onslide<1->{(y_1, y_1') & = (x^2, 2x)}                          \\
			\onslide<2->{(y_2, y_2') & = (\sin x, \cos x)}                   \\
			\\
			\onslide<3->{(y_3, y_3') & = (y_1^2, 2y_1y_1') = (x^4, 4x^3)}    \\
			\onslide<4->{(y_4, y_4') & = (y_1+y_1, y_1'+y_2')}               \\
			\onslide<4->{            & = (x^2+\sin x, 2x+\cos x)}            \\
			\onslide<5->{(y_5, y_5') & = (-y_2, -y_2') = (-\sin x, -\cos x)}
		\end{align*}
	\end{minipage}
\end{frame}

\begin{frame}
	\frametitle{Summary}
	\begin{itemize}
		\item Forward mode AD reuses gradients from upstreams. Therefore, this mode is useful for few-to-many mappings
		      $$f:\mathbb{R}^n\rightarrow \mathbb{R}^m, n\ll m$$
		\item Applications: sensitivity analysis, uncertainty quantification, etc.
		      \begin{itemize}
			      \item Consider a physical model $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$, let $x\in \RR^n$ be the quantity of interest (usually a low dimensional physical parameter), uncertainty propagation method computes the perturbation of the model output (usually a large dimensional quantity, i.e., $m\gg 1$)
			            $$f(x+\Delta x) \approx f(x) + f'(x) \Delta x$$
		      \end{itemize}

	\end{itemize}
\end{frame}





\section{Reverse Mode}

\begin{frame}
	\frametitle{Reverse Mode AD}
	$$\frac{df(g(x))}{dx} = \red{f'(g(x))} g'(x)$$
	\begin{itemize}
		\item Computing in the reverse order of forward computation.
		\item Each node in the computational graph
		      \begin{itemize}
			      \item \red{Aggregates} all the gradients from down-streams
			      \item \red{Back-propagates} the gradient to upstream nodes.
		      \end{itemize}
		      \begin{figure}[hbt]
			      \includegraphics[width=0.4\textwidth]{figures/rad}
		      \end{figure}

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Example: Reverse Mode AD}
	$$z=\sin(x_1+x_2) + x_2^2x_3$$
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.5\textwidth]{figures/bd1}
	\end{figure}


\end{frame}

\begin{frame}
	\frametitle{Example: Reverse Mode AD}
	$$z=\sin(x_1+x_2) + x_2^2x_3$$
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.5\textwidth]{figures/bd2}
	\end{figure}
\end{frame}


\begin{frame}
	\frametitle{Example: Reverse Mode AD}
	$$z=\sin(x_1+x_2) + x_2^2x_3$$
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.5\textwidth]{figures/bd3}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Example: Reverse Mode AD}
	$$z=\sin(x_1+x_2) + x_2^2x_3$$
	\begin{figure}[hbt]
		\centering
		\includegraphics[width=0.5\textwidth]{figures/bd4}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Summary}

	\begin{itemize}
		\item Reverse mode AD reuses gradients from down-streams. Therefore, this mode is useful for many-to-few mappings
		      $$f:\mathbb{R}^n\rightarrow \mathbb{R}^m, n\gg m$$
		\item Typical application:
		      \begin{itemize}
			      \item Deep learning: $n=$ total number of weights and biases of the neural network, $m=1$ (loss function).
			      \item Mathematical optimization: usually there are only a single objective function.
		      \end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Summary}

	\begin{itemize}
		\item In general, for a function $f:\RR^n \rightarrow \RR^m$
		      % Please add the following required packages to your document preamble:
		      % \usepackage{booktabs}
		      \begin{table}[]
			      \centering
			      \begin{tabular}{@{}llll@{}}
				      \toprule
				      Mode    & Suitable for ... & Complexity\footnote{$\mathrm{OPS}$ is a metric for complexity in terms of fused-multiply adds.} & Application      \\ \midrule
				      Forward & $m\gg n$         & $\leq 2.5\;\mathrm{OPS}(f(x))$                                                                  & UQ               \\
				      Reverse & $m\ll n$         & $\leq 4\;\mathrm{OPS}(f(x))$                                                                    & Inverse Modeling \\ \bottomrule
			      \end{tabular}
		      \end{table}


		\item There are also many other interesting topics
		      \begin{itemize}
			      \item Mixed mode AD: many-to-many mappings.
			      \item Computing sparse Jacobian matrices using AD by exploiting sparse structures.
		      \end{itemize}
	\end{itemize}
	{\scriptsize Margossian CC. A review of automatic differentiation and its efficient implementation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 2019 Jul;9(4):e1305.}
\end{frame}


\section{AD for Physical Simulation}

\begin{frame}
	\frametitle{The Demand for Gradients in Physical Simulation}

	\begin{figure}[hbt]
		\includegraphics[width=0.9\textwidth]{figures/illu.png}
	\end{figure}

	\begin{itemize}
		\item Solving nonlinear equations
		\item Uncertainty quantification/sensitivity analysis
		\item \red{Inverse problems}
	\end{itemize}

	\scriptsize{Image source: \url{https://mirams.wordpress.com/2016/11/23/uncertainty-in-risk-prediction/}, \url{http://fourier.eng.hmc.edu/e176/lectures/ch2/node5.html}}
\end{frame}


\begin{frame}
	\frametitle{Inverse Problem and Mathematical Optimization}

	\begin{itemize}
		\item Consider a bar under heating with a source term $f(x,t)$. The right hand side has fixed temperature and the left hand side is insulated.
		\item The governing equation for the temperature $u(x,t)$ is
		      \begin{align*}
			      \frac{\partial u({x}, t)}{\partial t}       & = \kappa(x)\Delta u({x}, t) + f({x}, t), \quad t\in (0,T), x\in \Omega \\
			      u(1, t)                                     & = 0 \quad t>0                                                          \\
			      \kappa(0)\frac{\partial u(0,t)}{\partial x} & = 0 \quad t>0
		      \end{align*}
		\item The diffusivity coefficient is given by
		      $$\kappa(x) = a + bx$$
		      where $a$ and $b$ are unknown parameters.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Inverse Problem and Mathematical Optimization}
	\begin{itemize}
		\item Goal: calibrate $a$ and $b$ from $u_0(t) = u(0, t)$
		      $$\kappa(x) = a + bx$$
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{figures/measure}
	\end{figure}

\end{frame}
\begin{frame}
	\frametitle{Inverse Problem and Mathematical Optimization}
	\begin{itemize}
		\item This problem is a standard inverse problem. We can formulate the problem as a PDE-constrained optimization problem
		      $$\begin{aligned}
				      \min_{a, b}\    & \int_{0}^t ( u(0, t)- u_0(t))^2 dt                                                                    \\
				      \mathrm{s.t.}\  & \frac{\partial u(x, t)}{\partial t} = \kappa(x)\Delta u(x, t) + f(x, t), \quad t\in (0,T), x\in (0,1) \\
				                      & -\kappa(0)\frac{\partial u(0,t)}{\partial x} = 0, t>0                                                 \\
				                      & u(1, t) = 0, t>0                                                                                      \\
				                      & u(x, 0) = 0, x\in [0,1]                                                                               \\
				                      & \kappa(x) = a x + b
			      \end{aligned}$$

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Numerical Partial Differential Equation}

	\begin{itemize}
		\item As with many physical modeling techniques, we discretize the PDE using numerical schemes. Here is a finite difference scheme for the PDE $k=1,2,\ldots,m, i=1,2,\ldots, n$
		      $$\frac{u^{k+1}_i-u^k_i}{\Delta t} = \kappa_i \frac{u^{k+1}_{i+1}+u^{k+1}_{i-1}-2u^{k+1}_i}{\Delta x^2} + f_i^{k+1}$$
	\end{itemize}
	\begin{minipage}[c]{0.49\textwidth}
		For initial and boundary conditions, we have
		\begin{align*}
			-\kappa_1 \frac{u_2^{k+1}-u_0^{k+1}}{2\Delta x} & = 0 \\
			u_{n+1}^{k+1}                                   & = 0 \\
			u_i^0                                           & = 0
		\end{align*}
	\end{minipage}~
	\begin{minipage}[c]{0.49\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/grid}
	\end{minipage}


\end{frame}

\begin{frame}
	\frametitle{Numerical Partial Differential Equation}

	\begin{itemize}
		\item Rewriting the equation as a linear system, we have
		      $$A(a,b)U^{k+1} = U^k + F^{k+1}, \quad U^k = \begin{bmatrix}u_1^k\\u_2^k\\\vdots \\u_n^k\end{bmatrix}$$
		      Here $\lambda_i = \kappa_i \frac{\Delta t}{\Delta x^2}$ and
			      {\footnotesize
				      \begin{equation*}
					      A(a,b) = \begin{bmatrix}
						      2\lambda_1+1 & -2\lambda_1  &                &            &                \\
						      -\lambda_2   & 2\lambda_2+1 & -\lambda_2     &            &                \\
						                   & -\lambda_3   & 2\lambda_3 + 1 & -\lambda_3 &                \\
						                   &              & \ddots         &            &                \\
						                   &              &                & \ddots     & -\lambda_{n-1} \\
						                   &              &                & -\lambda_n & 2\lambda_n+1
					      \end{bmatrix},\quad F^k = \Delta t \begin{bmatrix}
						      f_1^{k+1} \\
						      f_2^{k+1} \\
						      \vdots    \\
						      f_n^{k+1}
					      \end{bmatrix}
				      \end{equation*}
			      }

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Computational Graph for Numerical Schemes}

	\begin{itemize}
		\item The discretized optimization problem is
		      \begin{align*}
			      \min_{a, b} & \; \sum_{k=1}^m (u^k_1 - u_0( (k-1)\Delta t))^2      \\
			      \text{s.t.} & \; A(a,b)U^{k+1} = U^k + F^{k+1}, k = 1, 2,\ldots, m \\
			                  & \; U^0 = 0
		      \end{align*}
		\item The computational graph for the forward computation (evaluating the loss function) is
		      \begin{figure}[hbt]
			      \centering
			      \includegraphics[width=0.3\textwidth]{figures/heatcg}
		      \end{figure}

	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Implementation using an AD system}

	\begin{figure}[hbt]
		\includegraphics[width=0.7\textwidth]{figures/simulation}
	\end{figure}

\end{frame}


\section{AD Through Implicit Operators}

\begin{frame}
	\frametitle{Challenges in AD}


	\begin{minipage}[t]{0.49\textwidth}
		\vspace{-3cm}
		\begin{itemize}
			\item Most AD frameworks only deal with explicit operators, i.e., the functions that have analytical derivatives, or composition of these functions.
			\item Many scientific computing algorithms are \textcolor{red}{iterative} or \textcolor{red}{implicit} in nature.
		\end{itemize}
	\end{minipage}~
	\begin{minipage}[t]{0.49\textwidth}
		\includegraphics[width=1.0\textwidth]{figures/sim.png}
	\end{minipage}

	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	\begin{table}[]
		\begin{tabular}{@{}lll@{}}
			\toprule
			Linear/Nonlinear   & Explicit/Implicit & Expression   \\ \midrule
			Linear             & Explicit          & $y=Ax$       \\
			Nonlinear          & Explicit          & $y = F(x)$   \\
			\textbf{Linear}    & \textbf{Implicit} & $Ay = x$     \\
			\textbf{Nonlinear} & \textbf{Implicit} & $F(x,y) = 0$ \\ \bottomrule
		\end{tabular}
	\end{table}
\end{frame}


\begin{frame}
	\frametitle{Example}

	Consider a function $f:x\rightarrow y$, which is implicitly defined by
	$$F(x,y) = x^3 - (y^3+y) = 0$$
	The forward computation may consist of iterative algorithms, such as the Newton's method and the bisection method:

	\vspace{-0.5em}

	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $y^0 \gets 0$
			\State $k \gets 0$
			\While {$|F(x, y^k)|>\epsilon$}
			\State $\delta^k \gets F(x, y^k)/F'_y(x,y^k)$
			\State $y^{k+1}\gets y^k - \delta^k$
			\State $k \gets k+1$
			\EndWhile
			\State \textbf{Return} $y^k$
		\end{algorithmic}
	\end{minipage}~
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\begin{algorithmic}
			\State $a \gets -M$, $b \gets M$, $m \gets 0$
			\While {$|F(x, m)|>\epsilon$}
			\State $m \gets \frac{a+b}{2}$
			\If{$F(x, m)>0$}
			\State $b \gets m$
			\Else
			\State $a \gets m$
			\EndIf
			\EndWhile
			\State \textbf{Return} $m$
		\end{algorithmic}

	\end{minipage}

\end{frame}

\begin{frame}
	\frametitle{Example}

	\begin{itemize}
		\item An efficient way is to apply the \textcolor{red}{implicit function theorem}. For our example, $F(x,y)=x^3-(y^3+y)=0$, treat $y$ as a function of $x$ and take the derivative on both sides
		      $$3x^2 - 3y(x)^2y'(x)-y'(x)=0\Rightarrow y'(x) = \frac{3x^2}{3y(x)^2+1}$$
		      The above gradient is \textcolor{red}{exact}.
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Implicit Operators in Physical Modeling}
	\begin{itemize}
		\item Return to our bar problem, what if the material property is complex and has a temperature-dependent governing equation?
		      $$\frac{\partial u({x}, t)}{\partial t} = \red{\kappa(u)}\Delta u({x}, t) + f({x}, t), \quad t\in (0,T), x\in \Omega$$
		\item An implicit scheme is usually a nonlinear equation, and requires an iterative solver (e.g., the Newton-Raphson algorithm) to solve
		      $$\frac{u^{k+1}_i-u^k_i}{\Delta t} = \red{\kappa(u_i^{k+1})} \frac{u^{k+1}_{i+1}+u^{k+1}_{i-1}-2u^{k+1}_i}{\Delta x^2} + f_i^{k+1}$$
		\item Typical AD frameworks cannot handle this operator. We need to differentiate through implicit operators.
		\item This topic will be covered in a future lecture: \red{physics constrained learning}.
	\end{itemize}

\end{frame}



\section{Conclusion}

\begin{frame}
	\frametitle{Conclusion}

	\begin{itemize}
		\item What's covered in this lecture
		      \begin{itemize}
			      \item Reverse mode automatic differentiation;
			      \item Forward mode automatic differentiation;
			      \item Using AD to solver inverse problems in physical modeling;
			      \item Automatic differentiation through implicit operators.
		      \end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{What's Next}

	\begin{itemize}
		\item Physics constrained learning: inverse modeling using automatic differentiation through implicit operators;
		\item Neural networks and numerical schemes: substitute the unknown component in a physical system with a neural network and learn the neural network with AD;
		\item Implementation of inverse modeling algorithms in ADCME.
	\end{itemize}
\end{frame}


\end{document}

---
layout: slides
---

class: center, middle

# CME 216, ME 343 - Winter 2021

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: middle

# BFGS

This is one of the most efficient optimizers.

It belongs to the class of quasi-Newton methods.

Instead of computing the Hessian exactly (expensive), we use an approximation.

---
class: middle

The method is ingenious.

Assume we have multiple evaluations of the gradient (e.g., one evaluation per step).

We know that: 

$$\nabla^2 l \cdot p \approx \nabla l(x+p) - \nabla l(x)$$

---
class: middle

This can be used to progressively approximate $\nabla^2 l$.

---
class: middle

 Example: pick $p = \varepsilon \, e_i$. Then:

$$\nabla^2 l \cdot p = \varepsilon \, \nabla^2 l[:,i] \approx \nabla l(x + p) - \nabla l(x)$$

We get column $i$ of the Hessian matrix.

---
class: middle

We can gain information about $\nabla^2 l$ from differences like

$$\nabla l(x+p) - \nabla l(x)$$

Let's use this insight to build an algorithm to approximate $\nabla^2 l$.

---
class: middle

$$x_{k+1} = x_k + \alpha_k p_k$$

$$\nabla l_{k+1} \approx \nabla l_k + \nabla^2 l \cdot (\alpha_k p_k)$$

---
class: middle

Define:

$$s\_k = x_{k+1} - x_k = \alpha_k p_k$$

$$y\_k = \nabla l_{k+1} - \nabla l_k$$

---
class: middle

$$\nabla l_{k+1} \approx \nabla l_k + \nabla^2 l \cdot s_k$$

$B\_{k+1} \approx \nabla^2 l_{k+1}$

Secant equation: 

$$B_{k+1} s_k = y_k$$

---
class: middle

But recall that Newton's step is approximately:

$$ - B_k^{-1} g_k $$

So instead of working with the Hessian $B_k$, it's more efficient to work directly with its inverse $H_k = B_k^{-1}$.

---
class: middle

The secant equation becomes:

$B_{k+1} s_k = y_k$ &rarr;

$$H_{k+1} y_k = s_k$$

---
class: middle

How can we use this secant equation to approximate $[\nabla^2 l]^{-1} ?$

---
class: middle

Assume we have some approximation at step $k$, $H_k$.

We want to use 

$$H_{k+1} y_k = s_k$$

to find a better approximation $H\_{k+1}$ of $[\nabla^2 l]^{-1}\_{k+1}$.

---
class: middle

There are many $H_{k+1}$ that will solve the secant equation.

In BFGS we solve for

$$\text{argmin}_H \Vert H - H_k \Vert \text{ (with some suitable norm)} $$

subject to $H=H^T$ and $H y_k = s_k$.

---
class: middle

BFGS solution is:

$$H_{k+1} = (I-\rho_k s_k y_k^T) H_k (I-\rho_k y_k s_k^T) + \rho_k s_k s_k^T$$

$$\rho_k = \frac{1}{y_k^T s_k}$$

---
class: middle

Let's check that it is correct. 

---
class: middle

$$H_{k+1} = (I-\rho_k s_k y_k^T) H_k (I-\rho_k y_k s_k^T) + \rho_k s_k s_k^T$$

$H_{k+1} y_k = s_k$. Multiply to the right by $y_k$:

$$\rho_k s_k s_k^T y_k = \frac{s_k^T y_k}{y_k^T s_k} s_k = s_k$$

$$(I-\rho_k y_k s_k^T) y_k = y_k - \frac{s_k^T y_k}{y_k^T s_k} y_k = 0$$

---
class: middle

$I-\rho_k y_k s_k^T$ is a projection onto $\\{s_k\\}^\perp$ along $y_k$.

$$P = I-\rho_k y_k s_k^T$$

$$P^2 = P$$

---
class: middle

![:width 40%](bfgs_proj.png)

---
class: middle

~~~Python
while &Vert;g[k]&Vert; > eps:
    pk = -Hk*g[k]
    x[k+1] = x[k] + ak*pk # ak is obtained using a line search
    sk = x[k+1] - x[k]
    yk = g[k+1] - g[k]
    Update Hk using BFGS equation
    k &larr; k+1
~~~

---
class: middle

[scipy optimize BFGS](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html) implementation

---
class: middle

For more information see [Numerical Optimization](https://www.amazon.com/Numerical-Optimization-Operations-Financial-Engineering/dp/0387303030/ref=sr_1_1?crid=36S4G2AJP8JV7&dchild=1&keywords=nocedal+wright&qid=1612048642&sprefix=nocedal%2Caps%2C234&sr=8-1), by Nocedal and Wright, Springer, 2nd edition
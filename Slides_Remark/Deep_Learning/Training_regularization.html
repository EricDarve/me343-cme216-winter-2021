---
layout: slides
---

class: center, middle

# CME 216, ME 343 - Spring 2020

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: middle

One simple method to avoid overfitting is to reduce the complexity of the model.

---
class: middle

We chose a very simple example to illustrate these concepts.

To avoid overfitting in this case, we need to use a single `tanh` function (one output node in the hidden layer).

---
class: middle

Then we get:

![:width 40%](fig16.png) ![:width 40%](fig17.png)

---
class: middle

Simplifying the model may reduce the issue of overfitting but this does not imply that we will necessarily get the right solutions.

By restricting the structure of the DNN we are optimizing, we may oversimplify and end up with a DNN that cannot accurately reproduce the solution.

---
class: middle

Another approach consists in keeping the same DNN but using regularization. 

This means adding a new term to the loss function that penalizes large weights and biases.

---
class: middle

Regularization is usually done using either l1 or l2 regularization.

Consider some training data $(x_i,y_i)$ and a mean squared error loss function.

---
class: middle

With l2 regularization, we define the loss as

$\text{Loss} = \sum\_i (y\_i - \hat{y}\_i)^2 + \lambda\_1 \sum\_{ijl} (w\_{ij}^{(l)})^2$

$\hspace{5em} + \lambda\_2 \sum\_{il} (b\_i^{(l)})^2$

---
class: middle

$\lambda_1$ and $\lambda_2$ are the regularization factors

$w\_{ij}^{(l)}$ is the weight $(i,j)$ in layer $l$

$b\_i^{(l)}$ is the bias $i$ in layer $l$.

---
class: middle

In TensorFlow/Keras the [syntax](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer) is:

- `kernel_regularizer`: regularizer to apply a penalty on the layer's weights (kernel)
- `bias_regularizer`: penalty on the layer's bias
- `activity_regularizer`: penalty on the layer's output

---
class: middle

With regularization we ensure that weights are kept small and cannot diverge.

The accuracy is then much improved.

---
class: middle

After 10,000 iterations, we get

![:width 40%](fig18.png) ![:width 40%](fig19.png)

---
class: middle

Physically we can interpret the l2 regularization as adding a spring to the weights and biases so that they are always pulled back towards 0.

Indeed consider

$$\lambda\_1 \sum\_{ijl} (w\_{ij}^{(l)})^2$$

---
class: middle

Minus the gradient with respect to $w\_{ij}^{(l)}$ is equal to

$$- 2 \lambda\_1 w\_{ij}^{(l)}$$

This is a spring force where the "displacement" of the spring is $w\_{ij}^{(l)}$, and the stiffness $2 \lambda_1$.

It prevents $w\_{ij}^{(l)}$ from getting too big.

---
class: middle

The l1 regularization uses the following formula (with the l1 norm):

$\text{Loss} = \sum\_i (y\_i - \hat{y}\_i)^2 + \lambda\_1 \sum\_{ijl} |w\_{ij}^{(l)}|$

$\hspace{5em} + \lambda\_2 \sum\_{il} |b\_i^{(l)}|$

---
class: middle

The l1 regularization has a different effect. 

It makes the solution (weights and biases) sparser. 

That is, it tries to reproduce the training data with as few non-zero weights and biases as possible.
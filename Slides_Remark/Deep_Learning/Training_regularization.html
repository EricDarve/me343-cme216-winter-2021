---
layout: slides
---

class: center, middle

# CME 216, ME 343 - Winter 2021

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: middle

One simple method to avoid overfitting is to reduce the complexity of the model.

---
class: middle

We chose a very simple example to illustrate these concepts.

To avoid overfitting in this case, we need to use a single `tanh` function (one output node in the hidden layer).

---
class: middle

Then we get:

![:width 40%](dnn_small_loss.png) ![:width 40%](dnn_small.png)

---
class: middle

Simplifying the model may reduce the issue of overfitting but this does not imply that we will necessarily get the right
solutions.

By restricting the structure of the DNN we are optimizing, we may oversimplify and end up with a DNN that cannot
accurately reproduce the solution.

---
class: middle

Another approach consists in keeping the same DNN but using regularization.

This means adding a new term to the loss function that penalizes large weights and biases.

---
class: middle

Regularization is usually done using either l1 or l2 regularization.

Consider some training data $(x_i,y_i)$ and a mean squared error loss function.

---
class: middle

With l2 regularization, we define the loss as

$\text{Loss} = \sum\_i (y\_i - \hat{y}\_i)^2 + \lambda\_1 \sum\_{ijl} (w\_{ij}^{(l)})^2$

$\hspace{5em} + \lambda\_2 \sum\_{il} (b\_i^{(l)})^2$

---
class: middle

$\lambda_1$ and $\lambda_2$ are the regularization factors

$w\_{ij}^{(l)}$ is the weight $(i,j)$ in layer $l$

$b\_i^{(l)}$ is the bias $i$ in layer $l$.

---
class: middle

In TensorFlow/Keras the [syntax](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer) is:

- `kernel_regularizer`: regularizer to apply a penalty on the layer's weights (kernel)
- `bias_regularizer`: penalty on the layer's bias
- `activity_regularizer`: penalty on the layer's output

---
class: middle

L2

```Python
kernel_regularizer=tf.keras.regularizers.L2(l=1e-3)
bias_regularizer=tf.keras.regularizers.L2(l=1e-3)
```

---
class: middle

With regularization we ensure that weights are kept small and cannot diverge.

The accuracy is then much improved.

---
class: middle

2 layers of size 128

`tf.keras.regularizers.L2(l=1e-3)`

![:width 40%](dnn_regd_loss.png) ![:width 40%](dnn_regd.png)

---
class: middle

Physically we can interpret the l2 regularization as adding a spring to the weights and biases so that they are always
pulled back towards 0.

Indeed consider

$$\lambda\_1 \sum\_{ijl} (w\_{ij}^{(l)})^2$$

---
class: middle

Minus the gradient with respect to $w\_{ij}^{(l)}$ is equal to

$$- 2 \lambda\_1 w\_{ij}^{(l)}$$

This is a spring force where the "displacement" of the spring is $w\_{ij}^{(l)}$, and the stiffness $2 \lambda_1$.

It prevents $w\_{ij}^{(l)}$ from getting too big.

---
class: middle

The l1 regularization uses the following formula (with the l1 norm):

$\text{Loss} = \sum\_i (y\_i - \hat{y}\_i)^2 + \lambda\_1 \sum\_{ijl} |w\_{ij}^{(l)}|$

$\hspace{5em} + \lambda\_2 \sum\_{il} |b\_i^{(l)}|$

---
class: middle

The l1 regularization has a different effect.

It makes the solution (weights and biases) sparser.

That is, it tries to reproduce the training data with as few non-zero weights and biases as possible.

---
class: middle

L1

```Python
kernel_regularizer=tf.keras.regularizers.L1(l=1e-3)
bias_regularizer=tf.keras.regularizers.L1(l=1e-3)
```

---
class: middle

L1 + L2

```Python
kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01,l2=0.01)
bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01,l2=0.01)
```
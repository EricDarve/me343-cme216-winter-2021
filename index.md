---
layout: page
title: Stanford ME 343 / CME 216 homepage
---

**Machine Learning for Computational Engineering**

These is the web site for ME 343/CME 216 Machine Learning in Computational Engineering. This material was created by [Eric Darve](https://me.stanford.edu/people/eric-darve), with the [help](https://github.com/EricDarve/me343-cme216-winter-2021/commits/main) of course staff and students.

## Syllabus

[Syllabus](syllabus)

## Policy for late assignments

Extensions can be requested in advance for exceptional circumstances (e.g., travel, sickness, injury, COVID related issues) and for OAE-approved accommodations.

Submissions after the deadline and late by at most 2 days (+48 hours after the deadline) will be accepted with a 10% penalty. No submissions will be accepted 2 days after the deadline.

See [Gradescope](https://www.gradescope.com/courses/222525) for all the current assignments and their due dates. Post on [Slack](https://stanford-3kml.slack.com) if you cannot access the Gradescope class page. We will send you the 6-letter code.

## Class modules and learning material

### Python tutorials

- [Python setup guide](Python Setup Guide)
- [Google colab guide](Homework/google_colab)
- [Introduction to Python notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/Python/Python%20basics.ipynb)
- [Numpy demo notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/Python/Numpy%20tutorial.ipynb)
- [Python inheritance demo notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/Python/Inheritance%20demo.ipynb)
- [All the files including required text input files](https://github.com/EricDarve/me343-cme216-winter-2021/tree/main/Code/Python)

### Introduction to ML and SVM

*Module 1-Part 1*

- 1.1 Brief introduction to ML; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=089ae55c-cdbe-412f-816b-ab92013a5794); [Slides](Slides/ML_introduction/brief_intro.pdf)
- 1.2 Examples of machine learning; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e64f9a8c-79b0-4e50-aac8-ab93015a260a); [Slides](Slides/ML_introduction/examples_ML.pdf)
- 1.3 Supervised learning; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=21ddf854-4f08-4293-93d2-ab9301632240); [Slides](Slides/ML_introduction/supervised_learning.pdf)
- 1.4 Machine learning in engineering; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=54cdfec5-a2a8-4e77-9a07-ab930165b07a); [Slides](Slides/ML_introduction/ml_in_engineering.pdf)
- 1.5 Introduction to SVM; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=db1ae91e-dedd-4f35-9824-ab9201843632); [Slides](Slides/ML_introduction/SVM_introduction.pdf)
- [Reading Assignment 1](Reading Assignments/intro_svm)

*Module 1-Part 2*

- [Python setup guide](Python Setup Guide)
- [SVM Python notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/svm.ipynb)
- 1.6 scikit-learn; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=79e12251-3c02-4909-af9d-ab9300073284); [Slides](Slides/ML_introduction/scikit-learn.pdf)
- 1.7 Soft-margin; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=fc2de4ae-5cd7-4e44-83f6-ab93000d4526); [Slides](Slides/ML_introduction/softmargin.pdf)
- 1.8 Over-fitting; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=822d5262-4b24-4773-ba4f-ab9301207cce); [Slides](Slides/ML_introduction/overfitting.pdf)
- 1.9 Training and validation sets; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=de353d83-76ba-4b30-b509-ab930127f783); [Slides](Slides/ML_introduction/training_validation.pdf)
- 1.10 Kernel trick; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=fc753602-132a-46fe-bdba-ab95000e417b); [Slides](Slides/ML_introduction/kernel_trick.pdf)
- [Reading Assignment 2](Reading Assignments/svm_kernel)
- [Homework 1](Homework/HW1/HW1 Questions)

### Deep Learning

*Module 2*

- [TensorFlow Python notebook DNN regression demo](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/DNN_regression.ipynb)
- 2.1 Perceptron; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b9bd406a-ee67-4b42-8f32-ab9c0002fc79); [Slides](Slides/TensorFlow/perceptron.pdf)
- 2.2 MLP; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b42e0a97-2134-430c-930f-ab9c000c2e90); [Slides](Slides/TensorFlow/MLP.pdf)
- 2.3 TensorFlow/Keras; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=88d50df4-652b-45b4-9dd6-ab9d000a3d6a); [Slides](Slides/TensorFlow/TensorFlow_Keras.pdf)
- 2.4 Sequential API; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=4e156180-3c8a-41d0-aaa1-ab9d0014328d); [Slides](Slides/TensorFlow/TF_sequential_API.pdf)
- 2.5 Functional API; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=adaafcf8-8a66-442a-af1e-ab9d00105e88); [Slides](Slides/TensorFlow/TF_functional_API.pdf)
- [Subclassing Python notebook inheritance demo](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/Inheritance%20demo.ipynb)
- 2.6 Subclassing; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=dd3c47e3-9d52-4bf0-864d-ab9d012ec854); [Slides](Slides/TensorFlow/TF_subclassing.pdf)
- [Reading Assignment 3](Reading Assignments/TensorFlow)

*Module 3-Part 1*

- [Python notebook DNN regularization demo](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/DNN_regularization.ipynb)
- 3.1 Loss function; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=12b616b7-cd88-4a82-9e17-abaa018b54bf); [Slides](Slides/Deep_Learning/Loss_function.pdf)
- 3.2 Cross-entropy; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=dd267b8b-074f-498f-97ac-abab00064d2c); [Slides](Slides/Deep_Learning/Cross_entropy.pdf)
- 3.3 TensorFlow loss functions; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=9efdcf32-d4c0-47b1-a6dd-abab011f9ac6); [Slides](Slides/Deep_Learning/Tensorflow_loss_functions.pdf)
- 3.4 Backpropagation; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1d4539ca-dda9-4b0f-96fa-abab012469a4); [Slides](Slides/Deep_Learning/Backpropagation.pdf)
- 3.5 Backpropagation formula; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=19daa234-19fd-4e3b-8d38-abab0155f6db); [Slides](Slides/Deep_Learning/Backpropagation_formula.pdf)
- 3.6 Learning rate for training; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e2081efc-3573-4ec5-8b73-abae000720cd); [Slides](Slides/Deep_Learning/Learning_rate_for_training.pdf)
- 3.7 Empirical method for learning rate; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=136043d7-4023-44f7-9068-abae000f3249); [Slides](Slides/Deep_Learning/Empirical_method_for_learning_rate.pdf)
- 3.8 Overfitting; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e43f30a2-4e28-4005-8bd1-acba0005c6e8); [Slides](Slides/Deep_Learning/Overfitting.pdf)
- 3.9 DNN initializers; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=42c5036a-ef10-4a65-8563-acba000d9424); [Slides](Slides/Deep_Learning/DNN_initializers.pdf)
- 3.10 Regularization; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=04b1cf19-aa64-457e-a1fe-acba00186069); [Slides](Slides/Deep_Learning/Regularization.pdf)
- [Reading Assignment 4](Reading Assignments/deep_learning)
- [Homework 2](Homework/HW2/HW2 Questions)

*Module 3-Part 2*

- [Saddle point demo notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/Saddle%20points.ipynb)
- [ADAGRAD notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/Adagrad.ipynb)
- 3.11 Stochastic Gradient Descent; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=3f76c7ec-a601-4d4a-8860-abb30137998c); [Slides](Slides/Deep_Learning/3_11_Stochastic_Gradient_Descent.pdf)
- 3.12 Saddle points; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c1fda57c-2c84-454b-b5c6-abb3014bb0d3); [Slides](Slides/Deep_Learning/3_12_Saddle_points.pdf)
- 3.13 Momentum; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=ae3feaf2-5884-42c5-aa01-abb3016db5f9); [Slides](Slides/Deep_Learning/3_13_Momentum.pdf)
- 3.14 Adagrad; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=9c74557c-832b-41d7-8cfd-abb30189fc85); [Slides](Slides/Deep_Learning/3_14_Adagrad.pdf)
- 3.15 RMSProp and Adam; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=8986cd22-6e69-4733-976c-abb4000dc637); [Slides](Slides/Deep_Learning/3_15_RMSProp_and_Adam.pdf)
- [Reading Assignment 5](Reading Assignments/optimization)
- [Homework 3](Homework/HW3/HW3 Questions)

*Module 3-Part 3*

- 3.16 Trust region; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b1e21c0a-e691-4c37-b7d0-acc00185514a); [Slides](Slides/Deep_Learning/3_16_Trust_region.pdf)
- 3.17 BFGS; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=36ab1ffb-32cb-4b2e-8ffb-acc1000e6e66); [Slides](Slides/Deep_Learning/3_17_BFGS.pdf)
- 3.18 L-BFGS; [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=ff39ec4f-ad00-452e-ba18-acc1001b6554); [Slides](Slides/Deep_Learning/3_18_LBFGS.pdf)
- [Reading Assignment 6](Reading Assignments/second_order_optimization)
- [Homework 4](Homework/HW4/HW4 Questions)

### Physics Informed Machine Learning

*Module 4*

- [Automatic Differentiation in TensorFlow notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/TensorFlow%20AD.ipynb)
- [Physics Informed Learning with TensorFlow notebook](https://github.com/EricDarve/me343-cme216-winter-2021/blob/main/Code/PIML_advanced.ipynb)
- 4.1 Physics Informed Machine Learning (PIML); [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d18be679-ed55-46eb-97f7-acd0013af907); [Slides](Slides/AD/4_1_PIML.pdf)
- 4.2 TensorFlow Automatic Differentiation (TF_AD); [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=a6199206-5d09-4a9b-97d0-acd0016624d8); [Slides](Slides/AD/4_2_TF_AD.pdf)
- 4.3 Physics Informed ML using TensorFlow (TF_PIML); [Video](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=12017b1f-243f-4915-ab91-acd1000fc63a); [Slides](Slides/AD/4_3_TF_PIML.pdf)
- [Reading Assignment 7](Reading Assignments/PIML)
- [Homework 5](Homework/HW5/HW5 Questions)

*Module 5 and 6*

_Automatic differentiation, Physics informed learning using ADCME, inverse modeling_

The slides are assembled into two PDF files. Each lecture video covers one section in one of these PDF files.

- Module 5, Automatic Differentiation for Computational Engineering; [Videos](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx?folderID=d9be6b74-6c53-4772-8eb3-acd8016c0c0b); [Slides](Slides/AD/5_AD.pdf)
- Module 6, Inverse Modeling using ADCME; [Videos](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx?folderID=f2279765-086a-402c-b5be-acd801850337); [Slides](Slides/AD/6_Inverse.pdf)

### Final project

[Final project instructions](Homework/Final_project)

## Reading material

###  Books

- [Deep learning](http://www.deeplearningbook.org/) by Ian Goodfellow and Yoshua Bengio and Aaron Courville
- [Deep learning with Python](https://searchworks.stanford.edu/view/13216992) by Fran&ccedil;ois Chollet
- [Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow : concepts, tools, and techniques to build intelligent systems](https://searchworks.stanford.edu/view/13489354) by Aur&eacute;lien G&eacute;ron
- [Numerical optimization](https://searchworks.stanford.edu/view/6630751) by Jorge Nocedal and Stephen Wright
- [Fundamentals of deep learning : designing next-generation machine intelligence algorithms](https://searchworks.stanford.edu/view/12112250) by Nikhil Buduma
- [Elements of statistical learning](https://searchworks.stanford.edu/view/12458005) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman
- [Deep learning: an introduction for applied mathematicians](https://epubs.siam.org/doi/pdf/10.1137/18M1165748) by Catherine Higham and Desmond Higham
- [Machine learning: a probabilistic perspective](https://www.cs.ubc.ca/~murphyk/MLbook/) by Kevin Murphy (in [searchworks](https://searchworks.stanford.edu/view/13163347))
- [Deep learning illustrated: a visual, interactive guide to artificial intelligence](https://searchworks.stanford.edu/view/13463749) by Jon Krohn
- [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen
- [Foundations of machine learning](https://cs.nyu.edu/~mohri/mlbook/) by Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar
- [Neural networks and learning machines](https://searchworks.stanford.edu/view/8631715) by Simon Haykin
- [The matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) by Kaare Petersen and Michael Pedersen

### Video tutorials

- [Introduction to deep learning: concepts and fundamentals](https://searchworks.stanford.edu/view/13216564) by Laura Graesser
- [Introduction to deep learning models with TensorFlow: learn how to work with TensorFlow to create and run a TensorFlow graph, and build a deep learning model](https://searchworks.stanford.edu/view/13214579) by Lucas Adams
- [Deep learning with TensorFlow: applications of deep neural networks to machine learning tasks](https://searchworks.stanford.edu/view/13215423) by Jon Krohn

### Review papers

- LeCun, Bengio and Hinton, Deep learning, _Nature,_ 521:436-444, 2015
- Schmidhuber, Deep learning in neural networks: an overview, _Neural Networks,_ 61:85-117, 2015
- [Automatic differentiation in machine learning: a survey](https://arxiv.org/pdf/1502.05767.pdf) by At&#305;l&#305;m G&uuml;nes Baydin, Barak Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind
- [A review of the adjoint-state method for computing the gradient of a functional with geophysical applications](https://academic.oup.com/gji/article/167/2/495/559970) by R.-E. Plessix

### Online classes and tutorials

- [Introduction to Deep Learning](http://introtodeeplearning.com/), MIT
- [fast.ai](https://course.fast.ai/)
- [Machine Learning 2014-2015](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/), Oxford, by Nando de Freitas

### Links

- [ADCME](https://github.com/kailaix/ADCME.jl) ([wiki page](https://kailaix.github.io/ADCME.jl/dev/)) developed by Kailai Xu and Prof. Darve
- [DeepXDE](https://github.com/lululxvi/deepxde) developed by Lu Lu and Prof. Karniadakis
- [List of books and tutorials on ML](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md)
- [Online courses](https://github.com/josephmisiti/awesome-machine-learning/blob/master/courses.md)
- [TensorFlow notebooks](https://github.com/the-deep-learners/TensorFlow-LiveLessons) by Jon Krohn
- [TF2 notebooks](https://github.com/jonkrohn/tf2) by Jon Krohn
- [Deep learning illustrated notebooks](https://github.com/the-deep-learners/deep-learning-illustrated) by Jon Krohn
- [TensorFlow playground](http://playground.tensorflow.org/)
- [MNIST visualization](https://www.cs.ryerson.ca/~aharley/vis/conv/) by Adam Harley
- [Distill](https://distill.pub/), a journal for machine learning visualizations

<!--
## Material from Spring 2020

## Reading assignments

Module 1

1. [Introduction to ML and SVM](Reading Assignments/intro_svm)
1. [Soft-margins in SVM](Reading Assignments/svm_softmargin)
1. [Kernel trick](Reading Assignments/kernel_trick)

- [Module 1 Solutions](RA Solutions/RA1-3_solutions)

Module 2

{:start="4"}
1. [Perceptron](Reading Assignments/perceptron)
1. [MLP](Reading Assignments/MLP) (multi-layer perceptron)
1. [TensorFlow](Reading Assignments/TF)
1. [Subclassing](Reading Assignments/subclassing)

Module 3

{:start="8"}
1. [Loss function and cross entropy](Reading Assignments/loss)
1. [Loss functions in TF/Keras](Reading Assignments/TF_loss)
1. [The backpropagation algorithm](Reading Assignments/Backpropagation)
1. [Learning rate and overfitting](Reading Assignments/LR_overfitting)
1. [Initializers and regularizers](Reading Assignments/initializers_regularizers)
1. [SGD and saddle points](Reading Assignments/SGD)
1. [Momentum and ADAGRAD](Reading Assignments/Momentum)
1. [RMSProp and Adam](Reading Assignments/Adam)

- [Module 3 Solutions](RA Solutions/RA_Solution_module3)

Module 4

{:start="16"}
1. [Automatic differentiation overview](Reading Assignments/AD)
1. [Computational graph](Reading Assignments/ComplGraph)
1. [Forward and reverse modes](Reading Assignments/FwdRevMode)
1. [AD for physical simulation](Reading Assignments/ADPhys)
1. [AD through implicit operators](Reading Assignments/ImplicitOps)

- [Module 4 Solutions](RA Solutions/RA_Solution_module4)

Module 5

{:start="21"}
1. [Inverse problems](Reading Assignments/Inverse)
1. [Training for inverse problems](Reading Assignments/InverseTraining)
1. [Physics constrained learning](Reading Assignments/PCL)
1. [Physics-informed learning conclusion](Reading Assignments/InverseConclusion)

- [Module 5 Solutions](RA Solutions/RA_Solution_module5)

Module 6

{:start="25"}
1. [Generative Adversarial Networks](Reading Assignments/GAN)

## Programming Homework

- [Python setup guide](Python Setup Guide)
- [Homework 1](Homework/HW1 Questions) and [starter code](Homework/hw1_starter_code.zip) and [solution](HW Solutions/svm.ipynb)
- [Homework 2](Homework/HW2 Questions) and [starter code](Homework/hw2_starter_code.zip) and [solution](HW Solutions/hw2_solution.zip)
- [Homework 3](Homework/HW3 Questions) and [starter code](Homework/hw3_starter_code.zip) and [solution](HW Solutions/hw3_solution.zip)
- [Homework 4](Homework/HW4/HW4 Questions) and [starter code](Homework/HW4/hw4_starter_code.zip) and [solution writeup](HW Solutions/hw4_solution) and [solution files](HW Solutions/hw4_solution.zip)

## Final Project

[Instructions](Homework/Final Project)

## Lecture slides and code

The videos accompanying these lectures can be found on canvas under ["Course Videos."](https://canvas.stanford.edu/courses/118944/external_tools/3367)

Python tutorial

- [Tutorial code](https://github.com/EricDarve/cme216-spring-2020/tree/master/Code/Python)
- [Python introduction notebook](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/Python/Python%20basics.ipynb)
- [Numpy notebook](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/Python/Numpy%20tutorial.ipynb)

Module 1

_Introduction to Machine Learning and Support Vector Machines_

- [SVM code](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/svm.ipynb)
- [1.1 Brief introduction to machine learning](Slides/ML_introduction/brief_intro)
- [1.2 A few examples of machine learning](Slides/ML_introduction/examples_ML)
- [1.3 Supervised learning](Slides/ML_introduction/supervised_learning)
- [1.4 Machine learning in engineering](Slides/ML_introduction/ml_in_engineering)
- [1.5 Introduction to SVM](Slides/SVM_introduction/)
- [1.6 Scikit-learn](Slides/scikitlearn/scikit)
- [1.7 Soft-margin](Slides/scikitlearn/softmargin)
- [1.8 Overfitting](Slides/scikitlearn/overfitting)
- [1.9 Training and validation sets](Slides/scikitlearn/training_validation)
- [1.10 Kernel trick](Slides/scikitlearn/kernel_trick)

Module 2

_Deep Neural Networks and TensorFlow_

- [2.1 Perceptron](Slides/ANN/perceptron)
- [2.2 Artificial Neural Networks](Slides/ANN/MLP) (multilayer perceptron)
- [2.3 TensorFlow/Keras](Slides/TF_Keras/TF_Pytorch)
- [2.4 Sequential API](Slides/TF_Keras/TF_sequential)
- [2.5 Functional API](Slides/TF_Keras/TF_functional)
- [2.6 Subclassing](Slides/TF_Keras/subclassing)
- [DNN TensorFlow code](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/DNN_regression.ipynb)
- [Python inheritance example code](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/Inheritance%20demo.ipynb)

Module 3

_Deep Learning_

- [3.1 Loss function for regression and classification](Slides/Deep_Learning/Loss)
- [3.2 Cross-entropy](Slides/Deep_Learning/Cross_entropy)
- [3.3 TensorFlow loss functions](Slides/Deep_Learning/TF_loss)
- [3.4 Backpropagation](Slides/Deep_Learning/Backprop)
- [3.5 Backpropagation formula](Slides/Deep_Learning/Backprop_formula)
- [3.6 Learning rate for training](Slides/Deep_Learning/Learning_rate)
- [3.7 Empirical method for learning rate](Slides/Deep_Learning/Learning_rate_empirical)
- [3.8 Overfitting](Slides/Deep_Learning/Training_overfitting)
- [3.9 DNN initializers](Slides/Deep_Learning/Training_initializers)
- [3.10 Regularization](Slides/Deep_Learning/Training_regularization)
- [3.11 Stochastic Gradient Descent](Slides/Deep_Learning/SGD)
- [3.12 Saddle points](Slides/Deep_Learning/Saddle_points)
- [3.13 Momentum](Slides/Deep_Learning/Momentum)
- [3.14 Adagrad](Slides/Deep_Learning/Adagrad)
- [3.15 RMSProp and Adam](Slides/Deep_Learning/Adam)
- [Regularization for DNNs example code](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/DNN_regularization.ipynb)
- [Saddle points illustration code](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/Saddle%20points.ipynb)
- [ADAGRAD benchmark code](https://github.com/EricDarve/cme216-spring-2020/blob/master/Code/Adagrad.ipynb)

Module 4 and 5

_Physics informed learning, automatic differentiation, inverse modeling_

The slides are assembled into two PDF files. Each lecture will cover one section in one of these PDF files. The lecture videos are on [Canvas](https://canvas.stanford.edu/courses/118944/external_tools/3367).

- [Automatic Differentiation for Computational Engineering](Slides/AD/AD.pdf)
- [Inverse Modeling using ADCME](Slides/AD/Inverse.pdf)

## Contents of class

Highlights of topics to cover

**Supervised learning and SVM**

Module 1 (week 1 and 2, 4/6, 4/13)

- Supervised learning
- SVM; [scikit-learn](https://scikit-learn.org/stable/); kernel trick; radial basis functions
- Overfitting; underfitting; regularization

- Homework 1 (SVM homework)

**Deep learning**

Module 2 (week 3, 4/20)

- NN and DNN; layers; weights and biases; activation function; loss function; skipped: universal approximation theorems; [Montufar et al. (2014)](http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf)
- [TensorFlow](https://www.tensorflow.org/learn) and [Keras](https://www.tensorflow.org/guide/keras)

Module 3 Part 1 (week 4, 4/27)

- Forward and back-propagation
- Weight initialization
- Regularization; test and validation sets; hyperparameter optimization
- Regularization strategies
- Skipped; batch normalization

- Homework 2 (covid-19 modeling)

Module 3 Part 2 (week 5-6, 5/4, 5/11)

- Stochastic gradient methods; SGD, momentum; adaptive algorithms

If time allows: convolution nets; pooling; fully-connected nets; DNN and convnet architectures

**Physics-informed learning**

Module 4 and 5 (week 6-8, 5/11--5/25)

- Physics-based ML; PhysML
- DNN and numerical PDE solvers
- Automatic differentiation; forward and reverse mode AD; chain rule; computational graph
- Examples of numerical PDE solutions with ADCME
- Physics constrained learning

- Homework 3 (week 6; bathymetry)

**Generative deep networks**

Module 6 (week 9-10, 6/1)

- PhysGAN and ADCME
- GANs to generate samples from a given probability distribution
- Generator and discriminator networks; WGANs
- Skipped: autoencoders and variational autoencoders
- TensorFlow example

- Homework 4 (5/31; physics informed learning)

**Reinforcement learning**

Module 7

We won't have enough time to cover this topic unfortunately.

- Reinforcement learning; [Sutton and Barto](http://incompleteideas.net/book/the-book.html); [Mnih 2013](https://arxiv.org/abs/1312.5602)
- Temporal difference learning; deep Q-learning networks
- Policy gradients and actor-critic algorithms

-->

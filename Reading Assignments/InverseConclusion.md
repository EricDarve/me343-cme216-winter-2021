---
layout: page
title: Physics-informed learning conclusion
--- 

Write your answers in a PDF and upload the document on [gradescope](https://www.gradescope.com/courses/102338) for submission. Each question is worth 10 points. Post on [Slack](https://stanford.enterprise.slack.com/) for questions.

[Slides on ADCME and inverse modeling](https://ericdarve.github.io/cme216-spring-2020/Slides/AD/Inverse.pdf)

Late day policy: 1 late day with a 20% grade penalty.

See the [slide deck](https://ericdarve.github.io/cme216-spring-2020/Slides/AD/Inverse.pdf) covered in the videos below. See also slides [44](https://ericdarve.github.io/cme216-spring-2020/Slides/AD/Inverse.pdf#page=44) to [47](https://ericdarve.github.io/cme216-spring-2020/Slides/AD/Inverse.pdf#page=47) for examples of applications of ADCME. These are not in the videos.

The next questions refer to video [5.7 Training Methods: Summary.](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b895a404-8472-4571-907a-abb70046773d)

1. Which methods apply when observations are sparse?
1. Which methods do not require optimizing over the state variable $u$?

The next questions refer to video [5.8 ADCME Overview.](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1343874b-0082-4158-91be-abb70048284b)

{:start="3"}
1. Between TensorFlow (TF) eager and TensorFlow graph, which one has the best performance in terms of AD capabilities?
1. Beside ADCME, is there a Julia library which offers AD capabilities? Which one is it?
1. How does the performance of PyTorch compare with TF eager and TF graph?

The next question refers to video [	5.9 Physics informed learning conclusion.](https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6767f1c0-f504-48b2-9827-abb7004a251c)

{:start="6"}
1. What are the four training algorithms we covered in this module? See slide [50](https://ericdarve.github.io/cme216-spring-2020/Slides/AD/Inverse.pdf#page=50).
---
layout: page
title: Perceptrons
---

These questions refer to the slide deck "2.1 Perceptron". 

Write your answers in a PDF and upload the document on [gradescope](https://www.gradescope.com/courses/102338) for submission.

Each question is worth 10 points. The due date is given on [gradescope](https://www.gradescope.com/courses/102338). Post on [Slack](https://stanford.enterprise.slack.com/) for questions.

Late day policy: 1 late day with a 20% grade penalty.

1. To train a perceptron or multi-layer perceptron we need to differentiate the loss function with respect to the model parameters. Explain why this makes the Heaviside function unsuitable for training.
1. Find and prove the relation between sigmoid and tanh.
1. What happens when you choose a learning rate that is too small? Describe what happens when the learning rate is too large.

